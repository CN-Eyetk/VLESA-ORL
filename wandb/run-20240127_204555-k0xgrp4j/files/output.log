/disk/junlin/EmoSp/bart-our/-LIGHT-TRANS4/all_loss0.6_0.2_0.2_kl-lr_2e-05-Situ-Emoin-nopp-empp-no_fuse-bart-eosemo-role-emocat-stgcat-vae256-wo_comet-frz_stem124_II
vocab size = 50265
without comet
BartConfig {
  "_name_or_path": "facebook/bart-base",
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_emo_cross_attn": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "emo_from_eos": true,
  "emo_from_situ": false,
  "emo_loss_ratio": 0.2,
  "emo_out_loss_ratio": 0.2,
  "emo_use_cat_attn": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "force_bos_token_to_be_generated": false,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "intensity_vae": false,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "latent_dim": 256,
  "lstm_st_seq": false,
  "max_position_embeddings": 1024,
  "merge": false,
  "mixed_vae": false,
  "model_type": "bart",
  "n_emo_out": 28,
  "no_fuse": true,
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "prepend": false,
  "rl_emb_ratio": 0.6,
  "sample_strat_emb": false,
  "scale_embedding": false,
  "st_from_eos": true,
  "stg_use_cat_attn": true,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.2.2",
  "use_cache": true,
  "use_copy": false,
  "use_emb_prep": true,
  "use_emo_in_dist": true,
  "use_kl": true,
  "use_role_embed": true,
  "use_situ": true,
  "use_st_seq": false,
  "use_th_attn": false,
  "use_trans_mat": true,
  "use_vae": true,
  "vocab_size": 50265,
  "wo_comet": true,
  "wo_emo": false,
  "wo_stra": false
}
args.local_rank =  -1
168,926,796 total parameters.
168,926,796 training parameters.
Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.layers.0.attn_layer_norm_comet.weight', 'encoder.layers.0.attn_layer_norm_comet.bias', 'encoder.layers.1.attn_layer_norm_comet.weight', 'encoder.layers.1.attn_layer_norm_comet.bias', 'encoder.layers.2.attn_layer_norm_comet.weight', 'encoder.layers.2.attn_layer_norm_comet.bias', 'encoder.layers.3.attn_layer_norm_comet.weight', 'encoder.layers.3.attn_layer_norm_comet.bias', 'encoder.layers.4.attn_layer_norm_comet.weight', 'encoder.layers.4.attn_layer_norm_comet.bias', 'encoder.layers.5.attn_layer_norm_comet.weight', 'encoder.layers.5.attn_layer_norm_comet.bias', 'encoder.layers.5.muAttn.k_proj.weight', 'encoder.layers.5.muAttn.k_proj.bias', 'encoder.layers.5.muAttn.v_proj.weight', 'encoder.layers.5.muAttn.v_proj.bias', 'encoder.layers.5.muAttn.q_proj.weight', 'encoder.layers.5.muAttn.q_proj.bias', 'encoder.layers.5.muAttn.out_proj.weight', 'encoder.layers.5.muAttn.out_proj.bias', 'encoder.emotion_head.weight', 'encoder.emotion_head.bias', 'encoder.batchNorm_emotion.weight', 'encoder.batchNorm_emotion.bias', 'encoder.batchNorm_emotion.running_mean', 'encoder.batchNorm_emotion.running_var', 'encoder.batchNorm_strategy.weight', 'encoder.batchNorm_strategy.bias', 'encoder.batchNorm_strategy.running_mean', 'encoder.batchNorm_strategy.running_var', 'encoder.strategy_embedding.weight', 'encoder.multi_state_LayerNorm.weight', 'encoder.multi_state_LayerNorm.bias', 'encoder.strategy_head.weight', 'encoder.strategy_head.bias', 'encoder.trans_mat.emotion_embedding.weight', 'encoder.trans_mat.h_prior_emo.weight', 'encoder.trans_mat.h_prior_emo.bias', 'encoder.trans_mat.mu_priors.0.weight', 'encoder.trans_mat.mu_priors.0.bias', 'encoder.trans_mat.mu_priors.1.weight', 'encoder.trans_mat.mu_priors.1.bias', 'encoder.trans_mat.mu_priors.2.weight', 'encoder.trans_mat.mu_priors.2.bias', 'encoder.trans_mat.mu_priors.3.weight', 'encoder.trans_mat.mu_priors.3.bias', 'encoder.trans_mat.mu_priors.4.weight', 'encoder.trans_mat.mu_priors.4.bias', 'encoder.trans_mat.mu_priors.5.weight', 'encoder.trans_mat.mu_priors.5.bias', 'encoder.trans_mat.mu_priors.6.weight', 'encoder.trans_mat.mu_priors.6.bias', 'encoder.trans_mat.mu_priors.7.weight', 'encoder.trans_mat.mu_priors.7.bias', 'encoder.trans_mat.logvar_priors.0.weight', 'encoder.trans_mat.logvar_priors.0.bias', 'encoder.trans_mat.logvar_priors.1.weight', 'encoder.trans_mat.logvar_priors.1.bias', 'encoder.trans_mat.logvar_priors.2.weight', 'encoder.trans_mat.logvar_priors.2.bias', 'encoder.trans_mat.logvar_priors.3.weight', 'encoder.trans_mat.logvar_priors.3.bias', 'encoder.trans_mat.logvar_priors.4.weight', 'encoder.trans_mat.logvar_priors.4.bias', 'encoder.trans_mat.logvar_priors.5.weight', 'encoder.trans_mat.logvar_priors.5.bias', 'encoder.trans_mat.logvar_priors.6.weight', 'encoder.trans_mat.logvar_priors.6.bias', 'encoder.trans_mat.logvar_priors.7.weight', 'encoder.trans_mat.logvar_priors.7.bias', 'encoder.trans_mat.Dense_z_priors.0.weight', 'encoder.trans_mat.Dense_z_priors.0.bias', 'encoder.trans_mat.Dense_z_priors.1.weight', 'encoder.trans_mat.Dense_z_priors.1.bias', 'encoder.trans_mat.Dense_z_priors.2.weight', 'encoder.trans_mat.Dense_z_priors.2.bias', 'encoder.trans_mat.Dense_z_priors.3.weight', 'encoder.trans_mat.Dense_z_priors.3.bias', 'encoder.trans_mat.Dense_z_priors.4.weight', 'encoder.trans_mat.Dense_z_priors.4.bias', 'encoder.trans_mat.Dense_z_priors.5.weight', 'encoder.trans_mat.Dense_z_priors.5.bias', 'encoder.trans_mat.Dense_z_priors.6.weight', 'encoder.trans_mat.Dense_z_priors.6.bias', 'encoder.trans_mat.Dense_z_priors.7.weight', 'encoder.trans_mat.Dense_z_priors.7.bias', 'encoder.trans_mat.h_posterior_emo.weight', 'encoder.trans_mat.h_posterior_emo.bias', 'encoder.trans_mat.mu_posteriors.0.weight', 'encoder.trans_mat.mu_posteriors.0.bias', 'encoder.trans_mat.mu_posteriors.1.weight', 'encoder.trans_mat.mu_posteriors.1.bias', 'encoder.trans_mat.mu_posteriors.2.weight', 'encoder.trans_mat.mu_posteriors.2.bias', 'encoder.trans_mat.mu_posteriors.3.weight', 'encoder.trans_mat.mu_posteriors.3.bias', 'encoder.trans_mat.mu_posteriors.4.weight', 'encoder.trans_mat.mu_posteriors.4.bias', 'encoder.trans_mat.mu_posteriors.5.weight', 'encoder.trans_mat.mu_posteriors.5.bias', 'encoder.trans_mat.mu_posteriors.6.weight', 'encoder.trans_mat.mu_posteriors.6.bias', 'encoder.trans_mat.mu_posteriors.7.weight', 'encoder.trans_mat.mu_posteriors.7.bias', 'encoder.trans_mat.logvar_posteriors.0.weight', 'encoder.trans_mat.logvar_posteriors.0.bias', 'encoder.trans_mat.logvar_posteriors.1.weight', 'encoder.trans_mat.logvar_posteriors.1.bias', 'encoder.trans_mat.logvar_posteriors.2.weight', 'encoder.trans_mat.logvar_posteriors.2.bias', 'encoder.trans_mat.logvar_posteriors.3.weight', 'encoder.trans_mat.logvar_posteriors.3.bias', 'encoder.trans_mat.logvar_posteriors.4.weight', 'encoder.trans_mat.logvar_posteriors.4.bias', 'encoder.trans_mat.logvar_posteriors.5.weight', 'encoder.trans_mat.logvar_posteriors.5.bias', 'encoder.trans_mat.logvar_posteriors.6.weight', 'encoder.trans_mat.logvar_posteriors.6.bias', 'encoder.trans_mat.logvar_posteriors.7.weight', 'encoder.trans_mat.logvar_posteriors.7.bias', 'encoder.strat_cat_attn.V', 'encoder.strat_cat_attn.W.weight', 'encoder.emo_cat_attn.V', 'encoder.emo_cat_attn.W.weight', 'decoder.layers.0.encoder_attn_layer_norm_strategy.weight', 'decoder.layers.0.encoder_attn_layer_norm_strategy.bias', 'decoder.layers.0.encoder_attn_layer_norm_comet.weight', 'decoder.layers.0.encoder_attn_layer_norm_comet.bias', 'decoder.layers.0.encoder_attn_layer_norm_total.weight', 'decoder.layers.0.encoder_attn_layer_norm_total.bias', 'decoder.layers.1.encoder_attn_layer_norm_strategy.weight', 'decoder.layers.1.encoder_attn_layer_norm_strategy.bias', 'decoder.layers.1.encoder_attn_layer_norm_comet.weight', 'decoder.layers.1.encoder_attn_layer_norm_comet.bias', 'decoder.layers.1.encoder_attn_layer_norm_total.weight', 'decoder.layers.1.encoder_attn_layer_norm_total.bias', 'decoder.layers.2.encoder_attn_layer_norm_strategy.weight', 'decoder.layers.2.encoder_attn_layer_norm_strategy.bias', 'decoder.layers.2.encoder_attn_layer_norm_comet.weight', 'decoder.layers.2.encoder_attn_layer_norm_comet.bias', 'decoder.layers.2.encoder_attn_layer_norm_total.weight', 'decoder.layers.2.encoder_attn_layer_norm_total.bias', 'decoder.layers.3.encoder_attn_layer_norm_strategy.weight', 'decoder.layers.3.encoder_attn_layer_norm_strategy.bias', 'decoder.layers.3.encoder_attn_layer_norm_comet.weight', 'decoder.layers.3.encoder_attn_layer_norm_comet.bias', 'decoder.layers.3.encoder_attn_layer_norm_total.weight', 'decoder.layers.3.encoder_attn_layer_norm_total.bias', 'decoder.layers.4.encoder_attn_layer_norm_strategy.weight', 'decoder.layers.4.encoder_attn_layer_norm_strategy.bias', 'decoder.layers.4.encoder_attn_layer_norm_comet.weight', 'decoder.layers.4.encoder_attn_layer_norm_comet.bias', 'decoder.layers.4.encoder_attn_layer_norm_total.weight', 'decoder.layers.4.encoder_attn_layer_norm_total.bias', 'decoder.layers.5.encoder_attn_layer_norm_strategy.weight', 'decoder.layers.5.encoder_attn_layer_norm_strategy.bias', 'decoder.layers.5.encoder_attn_layer_norm_comet.weight', 'decoder.layers.5.encoder_attn_layer_norm_comet.bias', 'decoder.layers.5.encoder_attn_layer_norm_total.weight', 'decoder.layers.5.encoder_attn_layer_norm_total.bias', 'fuse_st_emo.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



































 38%|███████████████████████████████████████████▌                                                                      | 195/510 [01:10<01:46,  2.95it/s]




































 78%|████████████████████████████████████████████████████████████████████████████████████████▌                         | 396/510 [02:52<00:40,  2.78it/s]



















100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 510/510 [04:03<00:00,  2.10it/s]
















 17%|████████████████████                                                                                               | 89/510 [00:30<02:21,  2.98it/s]



































 57%|████████████████████████████████████████████████████████████████▌                                                 | 289/510 [02:13<01:14,  2.96it/s]




































 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 489/510 [03:55<00:07,  2.79it/s]



 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 506/510 [04:32<00:01,  2.70it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 510/510 [04:33<00:00,  1.87it/s]






























 35%|████████████████████████████████████████                                                                          | 179/510 [01:01<01:47,  3.09it/s]



































 74%|████████████████████████████████████████████████████████████████████████████████████▋                             | 379/510 [02:40<00:43,  3.00it/s]























100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 508/510 [03:56<00:00,  3.00it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 510/510 [03:56<00:00,  2.15it/s]











 13%|███████████████▎                                                                                                   | 68/510 [00:23<02:33,  2.87it/s]




































 53%|████████████████████████████████████████████████████████████▏                                                     | 269/510 [02:03<01:23,  2.90it/s]



































 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍         | 467/510 [03:42<00:14,  2.92it/s]








100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 508/510 [04:29<00:00,  2.95it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 510/510 [04:29<00:00,  1.89it/s]


























 31%|██████████████████████████████████▊                                                                               | 156/510 [00:53<02:00,  2.93it/s]




































 70%|████████████████████████████████████████████████████████████████████████████████                                  | 358/510 [02:34<00:51,  2.92it/s]


























 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 505/510 [03:56<00:01,  2.92it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 510/510 [03:58<00:00,  2.14it/s]








  9%|██████████▊                                                                                                        | 48/510 [00:16<02:38,  2.92it/s]



































 48%|███████████████████████████████████████████████████████▏                                                          | 247/510 [01:57<01:30,  2.90it/s]




































 88%|████████████████████████████████████████████████████████████████████████████████████████████████████▎             | 449/510 [03:36<00:21,  2.90it/s]










100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 510/510 [04:10<00:00,  2.03it/s]
  1%|▋                                                                                                                   | 3/510 [00:00<02:47,  3.03it/s]
























 27%|███████████████████████████████                                                                                   | 139/510 [00:47<01:59,  3.10it/s]















 45%|██████████████████████████████████████████████████▋                                                               | 227/510 [01:31<01:53,  2.49it/s]
Traceback (most recent call last):
  File "/home/lijunlin/lijunlin/ESCONV/main.py", line 328, in <module>
    global_step, tr_loss = train(args, logger, args.train_dataset, model, tokenizer)
  File "/home/lijunlin/lijunlin/ESCONV/BlenderEmotionalSupport.py", line 1219, in train
    backward_loss.backward()
  File "/home/lijunlin/miniconda3/envs/empchat/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/lijunlin/miniconda3/envs/empchat/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacty of 23.68 GiB of which 335.00 MiB is free. Including non-PyTorch memory, this process has 23.35 GiB memory in use. Of the allocated memory 20.74 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF